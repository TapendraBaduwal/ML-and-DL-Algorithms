{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part1.LinearRegression.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNNedYyU9d5lqwWGbVBsM5G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Math Behind Machine Learning Algorithms**\n","- ML algorithms are generally categorized into five major learning approaches: **Supervised Learning,Semi-supervised learning,Unsupervised Learning,Reinforcement Learning and Deep Learning.**"],"metadata":{"id":"087t5DTKDnab"}},{"cell_type":"markdown","source":["##**Supervised learning:**\n","-  In Supervised learning  the model is getting trained on a **labelled dataset**. A labelled dataset is one that has both **input and output parameters** and have  been tagged with labels identifying characteristics, properties or classifications.\n","\n","\n","- Supervised learning solve **mainly two types of problems i.e Regression and Classification types.**\n","\n","- Regression involves predicting a **continuous value,** while classification involves predicting a **discrete value.** For Examples,predicting the price of a house is a regression problem, whereas predicting whether an image is of a dog or a cat is a classification problem.\n","\n","- **Regression** is used to understand the relationship between dependent and independent variables. It is commonly used to make projections, such as for **sales revenue** for a given business, Weather forecasting, Market Trends etc. **Common regression algorithms are Linear Regression and Polynomial Regression are popular regression algorithms.**\n","\n","\n","- **Classification** algorithms are used when the output variable is categorical, which means there are two classes such as Yes-No, Male-Female, True-false, spam-not spam,phising-notphising etc. **Common classification algorithms are Logistic Regression,Naïve Bayes,Decision Trees, Random Forest,ADA BOOST, Support Vector Machines (SVM), K-Nearest Neighbor(KNN)**, which are described in more detail in future.\n","\n","\n"],"metadata":{"id":"Lj02uHrHa_GV"}},{"cell_type":"markdown","source":["##**Semi-supervised learning:**\n","- Semi-supervised machine learning is a combination of supervised  and unsupervised learning.It is suitable to use when we have a **small amount of labeled data and a large amount of unlabeled data**.\n","\n","- Firstly, it trains the model with less amount of training data similar to the supervised learning models. The training continues until the model gives accurate results.\n","\n","- Then, the algorithms use the unlabeled dataset with pseudo labels in the next step, and in this step the result may not be accurate.**Pseudo labelling is the process of using the labelled data model to predict labels for unlabelled data.**\n","\n","- Now, the labels from labeled training data and pseudo labels data are linked together.\n","\n","- The input data in labeled training data and unlabeled training data are also linked.\n","\n","- At the end, again train the model with the new combined input as did in the first step. It will reduce errors and improve the accuracy of the model.\n","\n","- Real world applications of Semi-supervised Learning  are **Speech Analysis,Web content classification,Text document classifier etc.** because it is difficult to find out large amount of labeled dataset.\n"],"metadata":{"id":"BkX0-i9wDSbc"}},{"cell_type":"markdown","source":["##**Unsupervised Learning:**\n","- It mainly deals with the **unlabelled dataset**. An unlabelled dataset is one that has  not been tagged with labels identifying characteristics, properties or classifications but only features to represent them.\n","- It is most commonly used to find **hidden patterns in large unlabeled datasets** through cluster analysis.\n","\n","- Its ability to discover similarities and differences in information make it the ideal solution for exploratory data analysis, cross-selling strategies, customer segmentation, and image recognition.\n","- No labels are given to the learning algorithm, leaving it on its own to find structure in its input. \n","\n","- Common unsupervised learning approaches are:**clustering association, and dimensionality reduction.**\n","\n","- **Clustering** is the task of dividing the  data points into a number of Cluster on the basis of similar properties and/or features.\n","Some Clustering algorithms are **K-means clusterin,Hierarchical clustering etc**\n","\n","- **Dimensionality reduction** is a technique used when the number of features, or dimensions, in a given dataset is too high.It is commonly used in the preprocessing data stage,**Principal component analysis (PCA) is a type of dimensionality reduction algorithm** High dimensionality may cause over-fitting ,low dimensionality may cause under-fitting and best dimensionality help in best fitting for model.\n","\n","- **Association rule**  finds interesting associations and relationships among large sets of data items.**Apriori,F-P Growth algorithm are based on association rule.** For example, if a customer buys bread, he most likely can also buy butter, eggs, or milk, so these products are stored within a shelf or mostly nearby.\n","\n","- **Over-fitting means when we have more/over  feature than required features because of more unnecessary features model prediction goes wrong.**\n","- **Under-fitting means when we have less/under feature because of less number of features model prediction goes wrong.**\n","- **Best-fitting means when we have best fitting feature then model prediction goes Right.**"],"metadata":{"id":"TJh5rH65UlS8"}},{"cell_type":"markdown","source":["##**Reinforcement Learning:**\n","\n","-  In reinforcement Learning the **intelligent agent (computer program) learns automatically from an environment using feedbacks**  by performing the actions and seeing the results of actions. For each good action, the agent gets positive feedback, and for each bad action, the agent gets negative feedback or penalty.\n","- The primary goal of an agent in reinforcement learning is to improve the performance by getting the maximum positive rewards.\n","- Since there is no labeled data, so the agent is bound to learn by its experience only with the process of **hit and trial methods.**\n","\n","- Reinforcement Learning is now a big help in **recommendation systems like news, music apps, and web-series apps like Netflix, etc.** These apps work as per customer preferences.\n","\n","- Approaches to Implement a Reinforcement Learning Algorithm are:**Value based,Policy based and Model based.**\n","- The **value-based** approach is about to find the optimal value function.\n","- In **Policy based** approch develop  policy that the action performed in every state helps to gain maximum reward point in the future.\n","- In **Model based** approch firstly create a virtual model for each environment. Then  agent learns to perform specific tasks in that particular environment.\n","\n","- Algorithms of Reinforcement Learning are:**Q-learning,SARSA,Deep Q-network etc.**\n","\n"],"metadata":{"id":"XpACBjdfPb0y"}},{"cell_type":"markdown","source":["##**Deep Learning:**\n","- Deep learning can be defined as the method of machine learning and artificial intelligence that is planned to take **actions like  human brain functions to make effective decisions.**\n","\n","- Deep learning is implemented with the help of **Neural Networks, and the idea behind the motivation of Neural Network is the biological neurons, which is nothing but a brain cell.**\n","\n","- Deep learning algorithms are dynamically made to **run through several layers of neural networks,** which are nothing but a set of decision-making networks that are pre-trained to serve a task.\n","\n","- Some important Deep Learning Algorithms are **Deep Neural Network (DNN),Convolutional Neural Networks (CNNs),Transformer etc.**\n","\n"],"metadata":{"id":"8gf0ZrBRy_8P"}},{"cell_type":"markdown","source":["#**Details About Algorithms**"],"metadata":{"id":"kLWOXKlc7g2Y"}},{"cell_type":"markdown","source":["#**(1)Linear regression(Supervised learning,Regression Based):**\n","- In Machine Learning Linear Regression is the supervised Machine Learning model in which the model finds the **best fit linear line between the independent and dependent variable.** \n","- A Linear Regression model’s main aim is to find the best fit linear line and the optimal values of **intercept and coefficients** such that the error is minimized.Error is the difference between the actual value and Predicted value.\n","\n","- The equation of the regression line is $y=\\beta_0+\\beta_1x$ (In general y=mx+c is equation of any straight line),Where $y$ is the dependent variable,$x$ is the independent variable, $\\beta_{0}$ is the $y$-intercept or bias, $\\beta_{1}$ is the coefficient for $x$ or slope of the regression line.\n","- Lets we have two points, ($x_1,y_1$) and ($x_2,y_2$), then **slop of line($\\beta_{1}$)=$\\frac{y_2-y_1}{x_2-x_1}$**\n","\n","\n"],"metadata":{"id":"TEplrQQC8i8m"}},{"cell_type":"markdown","source":["#**Regression is divided into major three types:**\n","\n"],"metadata":{"id":"3V6MMIDZ4edN"}},{"cell_type":"markdown","source":["##**(a)Simple linear regression:**\n","- The simplest linear regression equation with one dependent variable and one independent variable is **$y=\\beta_0+\\beta_1x$** , Where $y$ is the dependent variable,$x$ is the independent variable, $\\beta_{0}$ is the $y$-intercept or bias, $\\beta_{1}$ is the coefficient for $x$ or slope of the regression line."],"metadata":{"id":"8-XhfJXs_O3k"}},{"cell_type":"markdown","source":["##**(b)Multiple linear regression:**\n","- If more than one independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression.\n","\n","- The multiple linear regression equation is $y= \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n$ ,Where $\\beta_1$,$\\beta_2$, $\\cdots$, $\\beta_n$ are regression coefficients for the independent variables $x_{1}$,$x_{2}$ ,$\\cdots$,$x_{n}$ respectively."],"metadata":{"id":"Wtlf0ZQ7_ZmM"}},{"cell_type":"markdown","source":["##**(c)Polynomial linear regression:**\n","- Polynomial Regression is best to use when our data is **non-linear.**\n","\n","- The equation of polynomial becomes something like this $y= \\beta_0 + \\beta_1x_1 + \\beta_2x_2^2 + \\cdots + \\beta_nx_n^n$ ,Where $\\beta_1$,$\\beta_2$, $\\cdots$, $\\beta_n$ are regression coefficients for the independent variables $x_{1}$,$x_{2}^2$ ,$\\cdots$,$x_{n}^n$ respectively.\n","-  In Polynomial regression, the original features are converted into Polynomial features of required degree (2,3,..,n) and then modeled using a linear model.\n","- Polynomial Regression is a powerful technique to encounter the situations where a **quadratic, cubic or a higher degree nonlinear relationship exists.**\n","\n","- For Example non-linear relationship between the **Position levels and experiance** and the salaries.Higher the position levels and experiance higher will be the salaries and vice-versa.\n"],"metadata":{"id":"nqrOf2qK_ck0"}},{"cell_type":"markdown","source":["##**Evaluating  the Performance of a Regression Problem:**\n","\n","<figure align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1bIWED4D8y6x1RFONLoIVqfFfAvJYiQxx\" height=\"200px\", width=\"300px\"> \n","</figure>\n","\n","\n","The major error metrics that are commonly used for evaluating and reporting the performance of a regression model; they are:\n","\n","Lets we have actual output value of y is  $y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$\n","\n","Also we have predicted  output  value of y is  $\\hat{y_i} = \\beta_0 + \\beta_1x_i$\n","\n","where $\\epsilon_i$ is error and $x_i$ is positional independent input variables then we can use this as follows;\n","\n","**(a)Residual Sum of Squares (RSS):**A residual error is the difference between the Actual/observed output values $y_{i}$ and the Predicted output Values $\\hat{y_{i}}$ then, Residual error= $y_{i}-\\hat{y_{i}}$.\n","$$ \\text{RSS} = \\sum_{i=1}^{n} (y_{i} - \\hat{y_{i}})^{2} $$\n","\n","\n","Scikit-Learn does not give direct apply for RSS. we can use the **mean_squared_error** function times the number of data samples to obtain the RSS.\n","\n","If number of  data samples increased then RSS also increased which is not good for our model.To overcome this proablem we used Mean Squared Error (MSE) as below:\n","\n","\n","**(b)Mean Squared Error (MSE):**It is the average of the squared difference between the actual and predicted value.\n","\n"," In RSS if number of  data samples increased then RSS also increased which is not good for our model.To overcome this proablem we used Mean Squared Error (MSE) and calculated  by dividing the RSS by the number of sample data points as below;\n","$$ \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} (y_{i} - \\hat{y_{i}})^{2} $$\n","\n","Scikit-Learn provide **mean_squared_error function** for MSE.\n","\n","**(c)Root Mean Squared Error (RMSE):**Some times  the large value of errors are not suitable for our model, so we need to make it smaller by  taking the square root of the MSE is called Root Mean Squared Error (RMSE).\n","$$ \\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y_{i}})^{2}} $$\n","\n","Scikit-Learn does not give direct apply for RMSE, but we can used  by taking the square root of the MSE.\n","\n","**(d)Mean Absolute Error (MAE):**The MAE measures the average magnitude of the errors in the dataset, without considering their direction.\n","$$ \\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_{i} - \\hat{y_{i}}|$$\n","\n","\n"," Scikit-Learn **median_absolute_error** for MAE.\n","\n","The smaller the error,the better your model fits your data; the greater the error,the poorer your model fits your data.A value of error zero means your model is a best fit.\n","\n","**(e)R2(R-squared) Score:**The goodness of fit of regression models can be analyzed on the basis of R-square method. The more the value of r-square near to 1, the better is the model.\n","\n","Lets we have $SS_{res}$ as Residual Sum of Squares or Sum of Squared Errors :\n","\n","$$SS_{res} = \\sum_{i=1}^{n} ({y_i}-\\hat{y_i})^2$$\n","\n","Also we have $SS_{tot}$ as Residual Sum of average total:\n","\n","$$SS_{tot} = \\sum_{i=1}^{n} (y_i-\\bar{y})^2$$\n","\n","Then we can wrire R-squared($R^2$) as follows:\n","\n","R-squared($R^2$)=$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n","\n","$$R^2 = 1 - \\frac{\\sum_{i=1}^{n} ({y_i}-\\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i-\\bar{y})^2}$$\n","\n","\n","\n","As we increase the number of independent features in the model, the R2 value will also keep on increasing even though the independent features are not co-related with the dependent variable.\n","To prevent such scenarios, **we use Adjusted R2**\n","\n","\n","**(f)Adjusted R2:**\n","\n","As we increase the number of independent features in the model, the R2 value will also keep on increasing even though the independent features are not co-related with the dependent variable.\n","To prevent such scenarios, **we use Adjusted R2**\n","\n","$$Adjusted R^2 = 1- (1-R^2)\\frac{n-1}{n-p-1}$$\n","\n","Here,\n","\n","n=observations or data points\n","\n","p=penalizing factor\n","\n","The Adjusted-$R^2$ has a penalizing factor(p). It penalizes for adding independent variable that don’t contribute to the model in any way or are not correlated to the dependent variable.\n","\n","Here penalizing factor(p) means not co-related features should be subtract from total number of features to get accurate result."],"metadata":{"id":"Fqvplr17iTL1"}},{"cell_type":"markdown","source":["##**Ordinary Least Squares(OLS):**\n","- Now we need to calculate the value of $\\beta_{0}$= $y$-intercept or bias, $\\beta_{1}$ =coefficient for $x$ or slope of the regression line, for **Best fitting regression line.**\n","- To find the **best-fit regression line**, the model aims to predict **$y$** value such that the error difference between Actual value and predicted value is minimum.\n","\n","- OLS or Ordinary Least Squares is a method in Linear Regression for estimating the **unknown parameters($\\beta_{0}$,$\\beta_{1}$....$\\beta_{n}$)** by creating a model which will **minimize the sum of the squared errors** between the actual data and the predicted data.\n"],"metadata":{"id":"Dn0wGjSsKAq0"}},{"cell_type":"markdown","source":["##**OLS with Simple Linear Regression:**\n","\n","Lets we have actual output value of y is  $y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$\n","\n","Also we have predicted  output  value of y is  $\\hat{y_i} = \\beta_0 + \\beta_1x_i$\n","\n","where $\\epsilon_i$ is error and $x_i$ is positional independent input variables.\n","\n","To calculate  the value of $\\beta_{0}$ and $\\beta_{1}$ with best-fit regression line, we minimize the Residual Sum of Squares(RSS) also called Sum of Squared Errors (SSE). \n","\n","Sum of Squared Errors (SSE) can also be written as:\n","\n"," $$\\text{SSE} = \\sum_{i=1}^{n}(y_{i}-\\hat{y_{i}})^2 =\\sum_{i=1}^{n}(y_{i}-(\\beta_0+\\beta_1x_i))^2 $$\n","\n","  In above we replaced simple linear regression  equation $\\hat{y_i} = \\beta_0 + \\beta_1x_i$\n","\n","**Lets Calculate $\\beta_0$ :**\n","\n","Firstly taking Partial derivatives(Partial derivatives means doing derivatives with respect of single variables and other variable remain constant) with respect to $\\beta_0$:\n","\n","$$\\frac{\\partial\\ \\text{SSE}}{\\partial \\beta_0}  = \\frac{\\partial }{\\partial \\beta_0}\\sum(y_i-(\\beta_0+\\beta_1x_i))^2$$\n","\n","\n","Now lets apply  chain rule and power rule(apply chain rule for making base of derivatives same) then, we get:\n","\n","\n","$$=\\sum\\frac{\\partial }{\\partial (y_i-(\\beta_0+\\beta_1x_i)}(y_i-(\\beta_0+\\beta_1x_i))^2 .\\frac{\\partial }{\\partial \\beta_0}(y_i-(\\beta_0+\\beta_1x_i))$$\n","\n","After  applying chain rule and power rule we get:\n","\n"," $$=2\\sum(y_i-(\\beta_0+\\beta_1x_i))(\\frac{\\partial\\ \\text{$y_i$}}{\\partial \\beta_0}-\\frac{\\partial\\ \\text{$\\beta_0$}}{\\partial \\beta_0}-\\frac{\\partial\\ \\text{$\\beta_1$$x_i$}}{\\partial \\beta_0})$$\n","\n","\n","$$=2\\sum(y_i-(\\beta_0+\\beta_1x_i))(0-1-0)$$\n","\n","$$=-2\\sum(y_i-(\\beta_0+\\beta_1x_i))\\tag{i}$$\n","\n","\n","Since,our main aim is to reduce error towards zero($0$) so,lets set up the partial derivatives equal to $0$ for equation $(i)$.\n","\n","$$-2\\sum(y_i-(\\beta_0+\\beta_1x_i))  = 0\\tag{ii}$$\n","\n","\n","Now we are going to solve the above equation to find our required parameters($\\beta_0$)\n"," \n","\n"," **Solving equation (ii) for $\\beta_0$**\n","\n"," lets divide equation (ii) in both side by $-2$ then we get,\n","\n","$$\\sum(y_i-(\\beta_0+\\beta_1x_i))  = 0$$\n","\n","Apply  summation in each terms inside the bracket, then we get:\n","\n","$$\\sum y_i- \\sum \\beta_0 - \\sum \\beta_1x_i = 0$$\n","\n","\n","Since, $\\beta_0 $ and $\\beta_1$ are random variables and take  any random value. But the values they take are constant over the samples.With respect to summation over the samples dataset,$\\beta_0 $ and $\\beta_1$  are constants so they can come outside the summation as follows:\n","\n","$$\\sum y_i- \\beta_0 - \\beta_1\\sum x_i = 0$$\n","\n","\n","Since,$\\beta_0$ written as $n\\beta_0$  because  from $1$ to $n$  data points the sum of each intercept values of data points is $n$ times of $\\beta_0$  then, we write;\n","\n","$$n\\beta_0 = \\sum y_i- \\beta_1\\sum x_i$$\n","\n","lets dividing both sides by $n$,then  we get:\n","\n","$$\\beta_0 = \\frac{\\sum y_i}{n}- \\frac{\\beta_1\\sum x_i}{n}$$\n","\n","Since the sum of all values of $y's$ divided by $n$ gives the mean or average value and also the sum of all values of $x's$ divided by $n$ gives the mean or average value.\n","\n","Finally we get the value of $\\beta_0$ as follows:\n","\n","\n","$$\\beta_0 = \\overline{y}- \\beta_1\\overline{x}\\tag{iii}$$\n","\n","\n","**Similarly lets calculate $\\beta_1$**\n","\n","Again taking Partial derivatives with respect to $\\beta_1$\n","\n","$$\\frac{\\partial\\ {\\text{SSE}} }{\\partial \\beta_1} = \\frac{\\partial }{\\partial \\beta_1}\\sum(y_i-(\\beta_0+\\beta_1x_i))^2$$\n","\n","Now,Put the value of $\\beta_0$ from above equation(iii) then, we get:\n","\n","\n","$$ = \\frac{\\partial }{\\partial \\beta_1}\\sum(y_i-(\\overline{y}- \\beta_1\\overline{x}+\\beta_1x_i))^2$$\n","\n","\n","Now lets apply  chain rule and power rule(apply chain rule for making base of derivatives same) then, we get:\n","\n","$$ ={\\sum\\frac{\\partial }{\\partial ((y_i-(\\overline{y}- \\beta_1\\overline{x}+\\beta_1x_i))}(y_i-(\\overline{y}- \\beta_1\\overline{x}+\\beta_1x_i))^2.\\frac{\\partial }{\\partial \\beta_1}(y_i-(\\overline{y}- \\beta_1\\overline{x}+\\beta_1x_i))}$$\n","\n","After  applying chain rule and power rule we get:\n","\n"," $$=2\\sum(y_i-(\\overline{y}- \\beta_1\\overline{x}+\\beta_1x_i)).(\\frac{\\partial\\ \\text{$y_i$}}{\\partial \\beta_1}-\\frac{\\partial\\ \\text{$\\overline{y}$}}{\\partial \\beta_1}+\\frac{\\partial\\ \\text{$\\beta_1$$\\overline{x}$}}{\\partial \\beta_1}-\\frac{\\partial\\ \\text{$\\beta_1$$x_i$}}{\\partial \\beta_1})$$\n","\n","\n","$$=2\\sum[(y_i-\\overline{y})- \\beta_1(x_i-\\overline{x})].(0-0+\\overline{x}-x_i)$$\n","\n","$$=2\\sum[(y_i-\\overline{y})- \\beta_1(x_i-\\overline{x})].[-(x_i-\\overline{x})]$$\n","\n"," $$=-2\\sum[(y_i-\\overline{y})- \\beta_1(x_i-\\overline{x})].[(x_i-\\overline{x})]\\tag{iv}$$\n","\n"," Since,our main aim is to reduce error towards zero($0$) so,lets set up the partial derivatives equal to $0$ for equation (iv).\n","\n","$$-2\\sum[(y_i-\\overline{y})- \\beta_1(x_i-\\overline{x})].[(x_i-\\overline{x})]=0\\tag{v}$$\n","\n","lets divide equation (v) in both side by $-2$ then we get,\n","\n","$$\\sum[(y_i-\\overline{y})- \\beta_1(x_i-\\overline{x})].[(x_i-\\overline{x})]=0$$\n","\n","Lets multiply by last outside bracket value with inside other values then we get;\n","\n","$$\\sum[(y_i-\\overline{y})(x_i-\\overline{x})- \\beta_1(x_i-\\overline{x})(x_i-\\overline{x})]=0$$\n","\n","$$\\sum(y_i-\\overline{y})(x_i-\\overline{x})- \\beta_1\\sum(x_i-\\overline{x})^2=0$$\n","\n","$$\\beta_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\tag{vi}$$\n","\n","**Finally we get Best fitting Parameters $\\beta_0$ and $\\beta_1$:**\n","\n","In general we put **hats** symboll on our $\\beta_0$ and $\\beta_0$ parameters because parameters are estimates.\n","\n","$$\\hat{\\beta_0} = \\bar{y} - \\beta_1\\bar{x}$$\n","\n","\n","$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"nLRt-DIH8w8G"}},{"cell_type":"markdown","source":["##**Limitations of  Ordinary Least Squares(OLS):**\n","- In **OLS with Multiple Linear Regression**(use same calculation process like OLS with simple linear regression) we obtain the  parameters as:\n","$$\\hat{\\boldsymbol{\\beta}} =(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}$$\n","\n","-  Here, $\\mathbf{X}$ is in the form of matrix form.\n","- From above if  matrix is not a full rank matrix then  **inverse of matrix doen not exist**  in this case OLS is not suitables.\n","\n","- OLS requires the number of samples(n ) must be greater than the number of features(d).\n","- To overcome this proablems  we used **Gradient Descent.**\n","\n","\n","\n"],"metadata":{"id":"cSbeesziDWrt"}},{"cell_type":"markdown","source":["#**Gradient Descent**\n","\n","- Gradient Descent is an algorithm to minimize the **cost function(error function)**  by **optimizing(Best fitting) its parameters.**\n","\n","- The cost function($J$) is  the **sum of squared error multiplied by $\\frac{1}{2}$ to make the derivation easier.**\n","\n","- To find the **best-fit regression line**, the model aims to predict **$y$** value such that the error difference between Actual value and predicted value is minimum.\n","- So, it is very important to update the value of  $β_0$, $β_1$,$β_2$,....,$β_n$ , to reach the best value that **minimize the error** between Actual output value and  predicted output value.\n","\n","- It is done by a random selection of values of coefficient and then iteratively update the values to reach the minimum cost function.\n","\n","- Gradient Descent  is mostly used in Linear regression, logistical regression,Principal component analysis (PCA),neural network etc and other various  machine learning and deep learning.\n","\n","\n","<figure align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1ln_I8G7cq79Lzbn5dy4djr8jbeyrgdCq\" height=\"300px\", width=\"500px\"> \n","</figure>\n"],"metadata":{"id":"YQXm-90VjWe3"}},{"cell_type":"markdown","source":["####**Lets  we have initial values of Paremeters $β_0$, $β_1$,$β_2$,....,$β_n$ (which is Randomly Initialization)**\n","\n","\n","$$\\boldsymbol{\\beta} =\\begin{bmatrix}\n","\\beta_0 \\\\ \n","\\beta_1 \\\\\n","\\beta_2 \\\\\n","\\vdots \\\\\n","\\beta_n\n","\\end{bmatrix}$$\n","\n","####**Since we have a cost function as below:**\n","\n","Lets we have actual output value of y is  $y_i = \\beta_0+\\beta_1x_{i1} +\\beta_2x_{i2} +....+ \\beta_nx_{in} + \\epsilon_i$\n","\n","Also we have predicted  output  value of y is  $\\hat{y_i} = \\beta_0+\\beta_1x_{i1} +\\beta_2x_{i2} +....+ \\beta_nx_{in}$\n","\n","where $\\epsilon_i$ is error and $x_i$ is positional independent input variables.\n","\n","Then the Cost Function is written as:\n","\n","\n","\\begin{align}\n","J(\\beta_0, \\beta_1, \\beta_2,..., \\beta_n) &= \\frac{1}{2}\\sum_{i=1}^{n}({y_{i}}-\\hat{y_{i}})^2 \\\\ \n","&= \\frac{1}{2}\\sum_{i=1}^{n}(y_{i}-(\\beta_0+\\beta_1x_{i1} +\\beta_2x_{i2} +....+ \\beta_nx_{in}))^2\n","\\end{align}\n","\n","###**Taking Partial derivative of the cost function with respect to  $\\beta_0$ then we get:**\n","\n","$$\\frac{\\partial\\ \\text{J}}{\\partial \\beta_0}  = \\frac{1}{2}\\sum_{i=1}^{n}\\frac{\\partial }{\\partial \\beta_0}(y_{i}-\\hat{y_{i}})^2$$\n","\n","Now apply chain and power rules:\n","$$=\\frac{1}{2}\\ \\sum_{i=1}^{n}\\frac{\\partial ({y_{i}}-\\hat{y_{i}})^2}\n","{\\partial ({y_{i}}-\\hat{y_{i}})} \\times \\frac{\\partial ({y_{i}}-\\hat{y_{i}})}{\\partial \\beta_0}$$\n","\n","\n","$$=\\sum_{i=1}^{n}({y_i} - \\hat{y_i}) \\times \\frac{\\partial (y_i-(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} +...+ \\beta_nx_{in}))}{\\partial \\beta_0}$$\n","\n","$$=\\sum_{i=1}^{n}({y_i} - \\hat{y_i}) \\times (-1)$$\n","\n","$$=-\\sum_{i=1}^{n}({y_i} - \\hat{y_i})$$\n","\n","\n","Similarly ,Taking Partial derivative of the cost function with respect to  $\\beta_1$,$\\beta_2$,...,$\\beta_n$ then we get:\n","\n","$$\\frac{\\partial\\ \\text{J}}{\\partial \\beta_1} =-\\sum_{i=1}^{n}({y_i} - \\hat{y_i})x_{i1}$$\n","\n","$$\\frac{\\partial\\ \\text{J}}{\\partial \\beta_2} =-\\sum_{i=1}^{n}({y_i} - \\hat{y_i})x_{i2}$$\n","\\begin{align}\\dots \\\\\\dots \\\\\\end{align}\n","\n","$$\\frac{\\partial\\ \\text{J}}{\\partial \\beta_n} =-\\sum_{i=1}^{n}({y_i} - \\hat{y_i})x_{in}$$\n","\n","Finally we  can write in the matrix form as below:\n","$$\\frac{\\boldsymbol{\\partial J}}{\\boldsymbol{\\partial \\beta}} = \\begin{bmatrix}\n","\\frac{\\partial J}{\\partial \\beta_0} \\\\ \n","\\frac{\\partial J}{\\partial \\beta_1}\\\\\n","\\frac{\\partial J}{\\partial \\beta_2}\\\\\n","\\vdots \\\\\n","\\frac{\\partial J}{\\partial \\beta_n}\n","\\end{bmatrix} $$\n","\n","\n","###**Lets update the value of parameters(optimizing its parameters to get Best fitting regression line) until the cost function converges to its minimum value:**\n","\n","Repeat the process until cost function converges to its minimum value:\n","$$\\beta_0 :=\\beta_0 - \\alpha\\frac{\\partial J}{\\partial \\beta_0}$$\n","\n","$$\\beta_1 :=\\beta_1-\\alpha\\frac{\\partial J}{\\partial \\beta_1}$$\n","\n","$$\\beta_2 :=\\beta_2-\\alpha\\frac{\\partial J}{\\partial \\beta_2}$$\n","\n","$$\\vdots \\\\\n","\\beta_n :=\\beta_n-\\alpha\\frac{\\partial J}{\\partial \\beta_n}$$\n","\n","**Note:** The  slop may be +ve or -ve,the slop of line decides in which direction to moves.That help to decide whether we have to add or subtract  on old paremeters to get new paremeters.New gradient tells us the slope of our cost function at our current position and the direction we should move to update our parameters.\n","\n","\n","###**LEARNING RATE:**\n","Here, $\\alpha$ is learning rate which determine the size of the step that we take.\n","This parameter determines how fast or slow we move towards the optimal weights while minimizing cost function at every step. High learning rate could cover more area per step but would risk overshooting the minima; low learning rate would take virtually forever to reach the minma."],"metadata":{"id":"7VBYZm-jTMyj"}},{"cell_type":"markdown","source":["##**Regularization in Machine Learning**\n","\n","- Regularization is one of the most important concepts of machine learning. It is a technique to prevent the model from overfitting and underfitting.\n","\n","- **Over-fitting** means when we have more/over feature than required features because of more unnecessary features model prediction goes wrong.\n","\n","- **Under-fitting** means when we have less/under feature because of less number of features model prediction goes wrong.\n","\n","- **Best-fitting** means when we have best fitting feature then model prediction goes Right.\n","\n","- In regularization technique, we **reduce the magnitude of the features by keeping the same number of features.**\n","\n","- Since,Higher the value of coefficient  it leads to  computationally expensive and model is more complex to solve this proablem we need regularization technique.\n","\n","- It is a form of regression that shrinks(make smaller in size) the coefficient estimates towards zero.\n","\n","\n","###**There are mainly two types of regularization techniques, which are given below:**\n","\n","####**(a)Ridge Regression:**\n","- In this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. We can calculate it by multiplying with the lambda to the squared weight of each individual feature.\n","\n","- The equation for the cost function in ridge regression will be:\n","$$Ridge Cost Function=\\sum_{i=1}^{n} (y_i-\\hat{y_i})^2 + \\lambda \\sum_{j=1}^{d} \\boldsymbol{\\beta_j^2}$$\n","\n","Here, $\\lambda \\sum_{j=1}^{d} \\boldsymbol{\\beta_j^2}$ is Ridge Regression penalty.\n","\n","- we know the higher the value of coefficent that means this features is most important for our model,and lower the value of coefficent mens this feature is not more important for our model.\n","\n","- If we increase small amount of $\\lambda$(0.0001,0.01,0.1,1,2,3,4...etc) then the irrelevent features coefficent  near to 0,and only relevent feature having higher value of coefficent remain on the equation.\n","\n","\n","- As we can see from the above equation, if the values of λ tend to zero, the equation becomes the cost function of the linear regression model. Hence, for the minimum value of λ, the model will resemble the linear regression model.\n","\n","- Ridge regression is mostly used to reduce the overfitting in the model, and it includes all the features present in the model. It reduces the complexity of the model by shrinking the coefficients.\n","\n","####**(a)Lasso Regression:**\n","- It is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights.\n","\n","- Since it takes absolute values, hence, it can shrink the  some irrelevent  features slope exactly to 0,which help to reduce overfitting of irrelevent features, whereas Ridge Regression can only shrink it near to 0.\n","\n","- The equation for the cost function of Lasso regression will be:\n","$$Lasso Cost Function = \\sum_{i=1}^{n} (y_i-\\hat{y_i})^2 + \\lambda \\sum_{j=1}^{d} \\boldsymbol{\\mid{\\beta_j\\mid}}$$ \n","\n","Here, $\\lambda \\sum_{j=1}^{d} \\boldsymbol{\\mid{\\beta_j\\mid}}$ is Lasso Regression penalty.\n","\n","- we know the higher the value of coefficent that means this features is most important for our model,and lower the value of coefficent mens this feature is not more important for our model.\n","\n","- If we increase small amount of $\\lambda$(0.0001,0.01,0.1,1,2,3,4...etc) then the irrelevent features coefficent  exactly become 0 and and only relevent feature having higher value of coefficent remain on the equation.\n","\n","- Lasso regression helps to reduce the overfitting in the model as well as feature selection.\n","\n","\n","####**Note:Now we can apply Ordinary Least Squares(OLS) and Gradient Descent on the above  Cost Function of Ridge and Lasso Regression by taking Partial Derivaties on Cost function to get new Best fitting line with best Parameters.**\n","\n"],"metadata":{"id":"D7u1YXUzwGpL"}},{"cell_type":"markdown","source":["###**Lets Generate Random Regression Proablem:**"],"metadata":{"id":"ZGYGbjh8CjYu"}},{"cell_type":"code","source":["from sklearn.datasets import make_regression #Generate a random regression problem.\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.model_selection import cross_val_score #Evaluate a score by cross-validation\n","from sklearn.metrics import r2_score           #To check the performance of our module\n","from sklearn.linear_model import LinearRegression"],"metadata":{"id":"vNOlrMnoU1PF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Lets take random 100  regression  paroablem data points\n","X,y = make_regression(n_samples=100, n_features=1, n_informative=1, n_targets=1,noise=20,random_state=13)\n","plt.scatter(X,y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"wMh8LrEuU50u","executionInfo":{"status":"ok","timestamp":1645615994090,"user_tz":-345,"elapsed":538,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"cc402f64-fa56-4da7-9cc8-d0069a102095"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.collections.PathCollection at 0x7f31973bead0>"]},"metadata":{},"execution_count":49},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["##**Lets apply OLS:**\n","If we used **from sklearn.linear_model import LinearRegression** this function work on the mechanism of OLS,which is already setup in sklearn libararies.Also we can did this from scratch,here we can directly use modul."],"metadata":{"id":"WAXVyT2eQAzT"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n","lr = LinearRegression()   #creat linearRegression object."],"metadata":{"id":"hQVOZbj5VA9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr.fit(X_train,y_train)\n","print(lr.coef_)\n","print(lr.intercept_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"siKjboNuVEpu","executionInfo":{"status":"ok","timestamp":1645616005007,"user_tz":-345,"elapsed":779,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"82fb0d5a-3094-420e-e09f-79e44336c13a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[28.12597332]\n","-2.2710144261783825\n"]}]},{"cell_type":"code","source":["y_pred = lr.predict(X_test)\n","from sklearn.metrics import r2_score\n","r2_score(y_test,y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4eIByWPRVIbF","executionInfo":{"status":"ok","timestamp":1645616008476,"user_tz":-345,"elapsed":559,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"70c2114b-87da-4d0e-dda8-5d5eab5f4750"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6345158782661012"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["##**Lets apply Gradient Descent from Scratch:**\n","After applying Gradient Descent the result may be same as above or may be improve in some case."],"metadata":{"id":"l3sWUA4XQ9hf"}},{"cell_type":"code","source":["class GDRegressor:                       \n","    \n","    def __init__(self,learning_rate,epochs):\n","        self.m = 100                             #set randomly slop of features\n","        self.b = -11                             #set randomly intercept of line\n","        self.lr = learning_rate                  #we have to give learning rate for best fitting.\n","        self.epochs = epochs                     # number of passes of the entire training dataset the machine learning algorithm has completed.\n","        \n","    def fit(self,X,y):\n","        # calcualte the b and m using GD\n","        for i in range(self.epochs):              #lets use y=mx+b as a regression  line here\n","            loss_slope_b = - np.sum(y - self.m*X.ravel() - self.b)     #cost function after taking partial derivatives with respect to b.\n","            loss_slope_m = - np.sum((y - self.m*X.ravel() - self.b)*X.ravel()) ##cost function after taking partial derivatives with respect to m.\n","            \n","            self.b = self.b - (self.lr * loss_slope_b) #updating the value of b\n","            self.m = self.m - (self.lr * loss_slope_m) #updating the value of m\n","        print(self.m,self.b)\n","        \n","    def predict(self,X):\n","        return self.m * X + self.b"],"metadata":{"id":"IjlIRCjRVMT7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gd = GDRegressor(0.001,50)  #passing learning rate as 0.001 and epochs 50 times to pass tarning dataset to our algorithms\n","gd.fit(X_train,y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i7WSCbfJVQbn","executionInfo":{"status":"ok","timestamp":1645615716398,"user_tz":-345,"elapsed":435,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"e3c03944-a1ee-4374-cf3a-509d71d08b12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["29.579945932037905 -2.68522559856985\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import r2_score\n","y_pred = gd.predict(X_test)\n","r2_score(y_test,y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bf7sIYWeVUk9","executionInfo":{"status":"ok","timestamp":1645616075532,"user_tz":-345,"elapsed":513,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"bc9f405a-1b27-4ca5-e6a1-1c8e2725ad60"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6261024795140153"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["####**From above we see that the performance of our algorithms from OLS and  Gradient Descent  nearly same,we apply Gradient Descent  as per need we already discuss in above theory.**"],"metadata":{"id":"cNAlWLyea78a"}}]}