{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part13.DeepLearning.ipynb","provenance":[],"authorship_tag":"ABX9TyOTlbpVyaj3sdW782aUR+/k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**(13)Deep Neural Network(Deep learning Based)**\n","\n","\n","- Neural Networks are used to perform many complex tasks including Image Classification, Object Detection, Face Identification, Text Summarization, speech recognition, and the list is endless.\n","\n"],"metadata":{"id":"5qidykSkbIvC"}},{"cell_type":"markdown","source":["##**Tensorflow Library**\n","\n","- TensorFlow is a popular framework of machine learning and deep learning, developed by Google Team. It is a free and open source software library and designed in Python programming language.\n","\n","- Here, **Tensor is a multidimensional array**, Flow is used to define the **flow of data in operation.**\n","\n","- A tensor is a vector or a matrix of n-dimensional that represents all type of data. All values in a tensor hold similar data type with a known shape. The shape of the data is the dimension of the matrix or an array.\n","- TensorFlow provides amazing functionalities and services when compared to other popular deep learning frameworks. TensorFlow is used to create a large-scale neural network with many layers.\n"],"metadata":{"id":"_rVFHx96UhQ5"}},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"8ZSZ2Oi82rlz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["const1 = tf.constant([[1,2,3], [1,2,3]])\n","const2 = tf.constant([[3,4,5], [3,4,5]])\n","result = tf.add(const1, const2)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DDQdhNBj3Ir7","executionInfo":{"status":"ok","timestamp":1646934181456,"user_tz":-345,"elapsed":17,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"2c6d72bb-c721-4ea6-a7de-f894a32edcfb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[4 6 8]\n"," [4 6 8]], shape=(2, 3), dtype=int32)\n"]}]},{"cell_type":"code","source":["var1 = tf.Variable([[1, 2], [1, 2]])\n","var2 = tf.Variable([[3, 4], [3, 4]])\n","result = tf.multiply(var1, var2)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TuyRT0PD3lwt","executionInfo":{"status":"ok","timestamp":1646934530973,"user_tz":-345,"elapsed":17,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"06346850-a521-45c7-bc79-bdbc18052673"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[3 8]\n"," [3 8]], shape=(2, 2), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["##**Kears Library**\n","\n","- Keras is a fast, open-source, and easy-to-use Neural Network Library written in Python that runs at top of Theano or Tensorflow. Tensorflow provides low-level as well as high-level API, indeed Keras only provide High-level API.\n","\n","- Keras is very quick to make a network model. If you want to make a simple network model with a few lines, Python Keras can help you with that. Look at the Keras example below:"],"metadata":{"id":"i2bE-_CDTo0R"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense, Activation,Conv2D,MaxPooling2D,Flatten,Dropout\n","\n","model = Sequential()\n","model.add(Dense(64, activation='relu', input_dim=50)) #input shape of 50 Dense Layer\n","model.add(Dropout(0.5))  #Dropout Layer\n"],"metadata":{"id":"2oR2_KB6AlTx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Activation Function**\n","\n","- The choice of activation functions in Deep Neural Networks has a significant impact on the training dynamics and task performance.\n","- A neural network without an activation function is essentially just a linear regression model.\n","\n","- Thus we use a non linear transformation to the inputs of the neuron and this non-linearity in the network is introduced by an activation function.\n","\n","\n","<figure align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1OUu-CoT0f4HzKygZ7cKlohvpdY-IeDeg\" height=\"300px\", width=\"400px\"img>\n","\n","\n","$WeightedSum= w_1x_1 +w_2x_2 +...+w_nx_n +bias$\n","\n","After calculating Weighted Sum  we pass this values to Activtion function and this activation function generate particular value for the given node ,this value act as input for next node as so on.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"GZczbyg8AGx8"}},{"cell_type":"markdown","source":["##**(a)Binary Step Function**\n","-  If the input to the activation function is greater than a threshold, then the neuron is activated, else it is deactivated, i.e. its output is not considered for the next hidden layer.\n","\n","- Threshold is the cut off value of the function. So if you set it to 0.5, anything below it is a 0 output, and anything above is a 1 output.\n","\n","- The binary step function can be used as an activation function while creating a binary classifier.\n","- As you can imagine, this function will not be useful when there are multiple classes in the target variable\n","\n","\\begin{align}\n","        \\text{f}(x) = \\left\\{\n","        \\begin{array}{cl}\n","        0 &  x < 0 \\\\\n","        1 & x ≥ 0\n","        \\end{array}\n","        \\right.\n","    \\end{align}\n","\n","- Gradients are calculated to update the weights and biases during the backpropgation process. Since the gradient of the Step function is zero, the weights and biases don’t update.So we need to solve this proablem with another Activation function.\n","- f'(x)(Gradients) = 0, for all x"],"metadata":{"id":"oZg5HYKOQ4Cd"}},{"cell_type":"code","source":["def binary_step(x):\n","    if x<0:\n","        return 0\n","    else:\n","        return 1\n","binary_step(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uu4ixUIJQ9dI","executionInfo":{"status":"ok","timestamp":1647006968061,"user_tz":-345,"elapsed":42,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"91e2e174-5492-408b-d6d7-fa344b1760d1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["##**(b)Linear Function:**\n","- We saw the problem with the step function, the gradient of the function became zero. This is because there is no component of x in the binary step function. Instead of a binary function, we can use a linear function. \n","$$f(x)=ax+b$$\n","\n","$$f(x) = (weight * input) + bias$$\n","\n","- When we differentiate the function with respect to x, the result is the coefficient of x, which is a constant.\n","- f'(x)(Gradients) = a\n","\n","- Although the gradient here does not become zero, but it is a constant which does not depend upon the input value x at all. This implies that the weights and biases will be updated during the backpropagation process but the updating factor would be the same.\n","\n","- It’s not possible to use backpropagation as the derivative of the function is a constant and has no relation to the input x. \n","\n","- In this scenario, the neural network will not really improve the error since the gradient is the same for every iteration. "],"metadata":{"id":"HY8Vto8PDbTM"}},{"cell_type":"code","source":["def linear_function(x):\n","    return 4*x\n","linear_function(4)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VADA1p9hDNzH","executionInfo":{"status":"ok","timestamp":1647003387729,"user_tz":-345,"elapsed":16,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"ef81f981-f4d8-4669-fe3b-8627ad8a6843"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["##**(c)Non-Linear Sigmoid Activation Function**\n","\n","- Mathmatically Sigmoid Function can be express as follows:\n","$$f(x) = \\frac 1 {1+ e^{-{x}}} $$\n","\n","- The output will always ranges between 0 to 1\n","- The gradient values are significant for range -3 and 3 but the graph gets much flatter in other regions. This implies that for values greater than 3 or less than -3, will have very small gradients. As the gradient value approaches zero, the network is not really learning.\n","\n","- f'(x)(Gradients) = sigmoid(x)*(1-sigmoid(x))\n","\n","\n","- This can be addressed by scaling the sigmoid function which is exactly what happens in the tanh function"],"metadata":{"id":"6RQopSmtEbkU"}},{"cell_type":"code","source":["import numpy as np\n","def sigmoid_function(x):\n","    z = (1/(1 + np.exp(-x)))\n","    return z\n","sigmoid_function(7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_ANixI5GrZb","executionInfo":{"status":"ok","timestamp":1647004339895,"user_tz":-345,"elapsed":797,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"28112a75-e4ff-40a1-93e9-9df1c4a92807"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9990889488055994"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["##**(d)Tanh Function (Hyperbolic Tangent)**\n","\n","- The tanh function is very similar to the sigmoid function. The only difference is that it is symmetric around the origin. The range of values in this case is from -1 to 1. Thus the inputs to the next layers will not always be of the same sign. \n","\n","$$f(x) = \\frac {e^{x}- e^{-{x}}}{e^{x}+ e^{-{x}}} $$\n","\n","- The output of Tanh is zero centered with a range from -1 to 1.\n","-  Usually tanh is preferred over the sigmoid function since it is zero centered.\n","- The gradient of the tanh function is much steeper as compared to the sigmoid function.\n"],"metadata":{"id":"Z5azHeSwJkYC"}},{"cell_type":"code","source":["def tanh_function(x):\n","    z = (2/(1 + np.exp(-2*x))) -1\n","    return z\n","tanh_function(-1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jBQoLtViLTOj","executionInfo":{"status":"ok","timestamp":1647005496228,"user_tz":-345,"elapsed":516,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"e0918c57-dfbd-4aae-f917-bce2f72f03a5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.7615941559557649"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["##**(e)ReLU Function**\n","\n","- The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.\n","\n","- If the input is negative the function returns 0, but for any positive input, it returns that value back.\n","- ReLU function is non-linear around 0, but the slope is always either 0 (for negative inputs) or 1 (for positive inputs).\n","- The function is very fast to compute (Compare to Sigmoid and Tanh) it doesn’t calculate exponent\n","\n","- For the negative input values, the result is zero, that means the neuron does not get activated. Since only a certain number of neurons are activated, the ReLU function is far more computationally efficient when compared to the sigmoid and tanh function\n","\n","\n","$$f(x) = max(0,x)$$\n","\n","\n","\\begin{align}\n","        \\text{f}(x) = max\\left\\{\n","        \\begin{array}{cl}\n","        0 &  x ≤ 0 \\\\\n","        x & x > 0\n","        \\end{array}\n","        \\right.\n","    \\end{align}\n","\n"],"metadata":{"id":"ehdA35v4KgES"}},{"cell_type":"code","source":["def relu_function(x):\n","    if x<0:\n","        return 0\n","    else:\n","        return x\n","\n","relu_function(7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SkgZbIXYOQ_R","executionInfo":{"status":"ok","timestamp":1647006287851,"user_tz":-345,"elapsed":567,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"7c82e502-f95c-4653-8984-0254f32b7adf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["##**Total Error**\n","\n","\\begin{align}\n","        E_{total} = \\sum{\\frac12}(TargetValueFromActivationFunction - ActualOutputValueFromActivationFunction)^2\n","    \\end{align}"],"metadata":{"id":"8JAbZC80nvTQ"}},{"cell_type":"markdown","source":["##**Update the weights to Reduce  Total Error** \n","\n","- To update the weight, we calculate the error correspond to each weight with the help of a total error. The error on weight w is calculated by differentiating(taking derivatives) total error with respect to w.\n","\n","- After that  we calculate new weight as follows:\n","\n","$$W_0:=w_0 - \\alpha\\frac{\\partial E_{total}}{\\partial w_0}$$\n","\n","$$W_1:=w_1 - \\alpha\\frac{\\partial E_{total}}{\\partial w_2}$$\n","\n","$$W_2:=w_2 - \\alpha\\frac{\\partial E_{total}}{\\partial w_3}$$\n","\n","$$\\vdots \\\\\n","W_n:=w_n- \\alpha\\frac{\\partial E_{total}}{\\partial w_n}$$\n","\n","\n","\n","- Here, $\\alpha$ is learning rate"],"metadata":{"id":"ih2E0_nJu60T"}},{"cell_type":"markdown","source":["##**DNN  model using Keras**"],"metadata":{"id":"vYho4-ZDaCtt"}},{"cell_type":"code","source":["from google.colab import drive     #mount your Google Drive in your virtual machine(VM).\n","drive.mount('/gdrive')              #Access  the data  drive because of different server of colab and drive."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmOfLXI4ahps","executionInfo":{"status":"ok","timestamp":1647030798677,"user_tz":-345,"elapsed":4165,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"71f56b3e-c576-4a14-f3b1-76d7a7943ec6"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"_5EyZLd2av9V","executionInfo":{"status":"ok","timestamp":1647030800830,"user_tz":-345,"elapsed":12,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["pima=pd.read_csv('/gdrive/My Drive/ML Project /Feature Engineering /4.ML Algorithms/diabetes.csv',quoting=3)\n","                                 #Read data file with path location step by step path location from My Drive."],"metadata":{"id":"SDO7MFBbaqx-","executionInfo":{"status":"ok","timestamp":1647030804034,"user_tz":-345,"elapsed":492,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["pima.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"7S787Odza0VZ","executionInfo":{"status":"ok","timestamp":1647030806488,"user_tz":-345,"elapsed":502,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"7f892db7-68ce-47ff-b260-5b3f260e7adb"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n","0            6      148             72             35        0  33.6   \n","1            1       85             66             29        0  26.6   \n","2            8      183             64              0        0  23.3   \n","3            1       89             66             23       94  28.1   \n","4            0      137             40             35      168  43.1   \n","\n","   DiabetesPedigreeFunction  Age  Outcome  \n","0                     0.627   50        1  \n","1                     0.351   31        0  \n","2                     0.672   32        1  \n","3                     0.167   21        0  \n","4                     2.288   33        1  "],"text/html":["\n","  <div id=\"df-1aa12b5e-88d2-4ae7-a4bd-c2635c69eab7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1aa12b5e-88d2-4ae7-a4bd-c2635c69eab7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1aa12b5e-88d2-4ae7-a4bd-c2635c69eab7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1aa12b5e-88d2-4ae7-a4bd-c2635c69eab7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["#split dataset in features and target variable\n","feature_cols = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin', 'BMI','DiabetesPedigreeFunction','Age']\n","X = pima[feature_cols] # Features/independent variables\n","y = pima.Outcome # Target variable/dependent variables\n","\n","\n","# or Also we can write above code as this also. \n","X = pima.drop('Outcome', axis=1)   #Features/independent variables\n","y = pima['Outcome']               # Target variable/dependent variables"],"metadata":{"id":"GRTMO6GobqID","executionInfo":{"status":"ok","timestamp":1647030810072,"user_tz":-345,"elapsed":437,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["Let's split dataset by using function train_test_split()."],"metadata":{"id":"gR8tC4Xgkp4A"}},{"cell_type":"code","source":["# split X and y into training and testing sets\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.35,random_state=42)"],"metadata":{"id":"RGRJJNW8hk_y","executionInfo":{"status":"ok","timestamp":1647030823661,"user_tz":-345,"elapsed":501,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["Since, we have different range of features so we need to apply Feature Scaling techniques to bring features in same scale."],"metadata":{"id":"Dacz7jLrktv2"}},{"cell_type":"code","source":["#Feature Scaling techniques to bring features in same scale.\n","from sklearn.preprocessing import RobustScaler # Or we can also use StandardScaler,MinMaxScaler depending on the dataset.\n","rb = RobustScaler()  \n","X_train = rb.fit_transform(X_train)\n","X_test = rb.transform(X_test)\n","\n","pd.DataFrame(X_train)   ##Convert numpy array generated by sklearn libraries to orginal dataframe.\n","pd.DataFrame(X_test)    ##Convert numpy array generated by sklearn libraries to orginal dataframe."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"YeNBYUmtkwRk","executionInfo":{"status":"ok","timestamp":1647030827175,"user_tz":-345,"elapsed":518,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"3a1e2cc4-5443-4d3a-cf58-881379e00af8"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       0         1       2        3         4         5         6         7\n","0    0.6 -0.481928 -0.8750  0.31250  1.209486  0.210526  0.131191  0.823529\n","1   -0.2 -0.144578  0.1875  0.28125 -0.292490  0.389474 -0.623829 -0.470588\n","2   -0.2 -0.240964 -0.5000 -0.71875 -0.292490 -0.126316 -0.597055 -0.470588\n","3    1.0 -0.265060  0.5000 -0.71875 -0.292490 -0.778947  1.271754  0.294118\n","4    0.8  0.433735  1.1250 -0.71875 -0.292490 -0.221053 -0.457831  1.235294\n","..   ...       ...     ...      ...       ...       ...       ...       ...\n","264  1.8 -0.289157  0.5000 -0.71875 -0.292490 -0.884211 -0.653280  0.882353\n","265 -0.2 -0.433735 -0.5000  0.00000 -0.292490 -0.242105 -0.034806 -0.470588\n","266  1.2  0.144578 -0.1250  0.31250  2.885375  0.357895 -0.265060  0.294118\n","267 -0.6 -0.554217 -0.5000  0.50000  0.537549  1.326316 -0.040161 -0.411765\n","268 -0.4 -0.939759  0.1875  0.21875 -0.292490  0.000000  0.040161 -0.411765\n","\n","[269 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-6007ff5e-49e8-4bcd-92b5-34d4006770ee\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.6</td>\n","      <td>-0.481928</td>\n","      <td>-0.8750</td>\n","      <td>0.31250</td>\n","      <td>1.209486</td>\n","      <td>0.210526</td>\n","      <td>0.131191</td>\n","      <td>0.823529</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.2</td>\n","      <td>-0.144578</td>\n","      <td>0.1875</td>\n","      <td>0.28125</td>\n","      <td>-0.292490</td>\n","      <td>0.389474</td>\n","      <td>-0.623829</td>\n","      <td>-0.470588</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.2</td>\n","      <td>-0.240964</td>\n","      <td>-0.5000</td>\n","      <td>-0.71875</td>\n","      <td>-0.292490</td>\n","      <td>-0.126316</td>\n","      <td>-0.597055</td>\n","      <td>-0.470588</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>-0.265060</td>\n","      <td>0.5000</td>\n","      <td>-0.71875</td>\n","      <td>-0.292490</td>\n","      <td>-0.778947</td>\n","      <td>1.271754</td>\n","      <td>0.294118</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.8</td>\n","      <td>0.433735</td>\n","      <td>1.1250</td>\n","      <td>-0.71875</td>\n","      <td>-0.292490</td>\n","      <td>-0.221053</td>\n","      <td>-0.457831</td>\n","      <td>1.235294</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>264</th>\n","      <td>1.8</td>\n","      <td>-0.289157</td>\n","      <td>0.5000</td>\n","      <td>-0.71875</td>\n","      <td>-0.292490</td>\n","      <td>-0.884211</td>\n","      <td>-0.653280</td>\n","      <td>0.882353</td>\n","    </tr>\n","    <tr>\n","      <th>265</th>\n","      <td>-0.2</td>\n","      <td>-0.433735</td>\n","      <td>-0.5000</td>\n","      <td>0.00000</td>\n","      <td>-0.292490</td>\n","      <td>-0.242105</td>\n","      <td>-0.034806</td>\n","      <td>-0.470588</td>\n","    </tr>\n","    <tr>\n","      <th>266</th>\n","      <td>1.2</td>\n","      <td>0.144578</td>\n","      <td>-0.1250</td>\n","      <td>0.31250</td>\n","      <td>2.885375</td>\n","      <td>0.357895</td>\n","      <td>-0.265060</td>\n","      <td>0.294118</td>\n","    </tr>\n","    <tr>\n","      <th>267</th>\n","      <td>-0.6</td>\n","      <td>-0.554217</td>\n","      <td>-0.5000</td>\n","      <td>0.50000</td>\n","      <td>0.537549</td>\n","      <td>1.326316</td>\n","      <td>-0.040161</td>\n","      <td>-0.411765</td>\n","    </tr>\n","    <tr>\n","      <th>268</th>\n","      <td>-0.4</td>\n","      <td>-0.939759</td>\n","      <td>0.1875</td>\n","      <td>0.21875</td>\n","      <td>-0.292490</td>\n","      <td>0.000000</td>\n","      <td>0.040161</td>\n","      <td>-0.411765</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>269 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6007ff5e-49e8-4bcd-92b5-34d4006770ee')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6007ff5e-49e8-4bcd-92b5-34d4006770ee button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6007ff5e-49e8-4bcd-92b5-34d4006770ee');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["###**Define Keras Model**\n","\n","- Model in Keras always defines as a sequence of layers. It means that we initialize the sequence model and add the layers one after the other which is executed as the sequence of the list.\n","\n","- The thing which you need to take care of is the first layer has the right number of input features which is specified using the **input_dim** parameter.\n","\n","\n","- The first layer has 12 neurons and activation function as relu.\n","- The second hidden layer has 8 neurons and activation function as relu.\n","\n","- Finally, at the output layer, we use 1 unit and activation as sigmoid because it is a binary classification problem.\n"],"metadata":{"id":"S0L509YucZlk"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense\n","model = Sequential()                                 #Model in Keras always defines as a sequence of layers so creat  object of it.\n","model.add(Dense(12, input_dim=8, activation=\"relu\")) #First layer has 12 neurons,input_dim=8 means we have 8 independent features.\n","model.add(Dense(8, activation=\"relu\"))               #The second hidden layer has 8 neurons and activation function as relu.\n","model.add(Dense(6, activation=\"relu\"))             #The third hidden layer has 8 neurons and activation function as relu.\n","model.add(Dense(1, activation=\"sigmoid\"))      #output layer,has 1 neurons,with sigmoid because it is a binary classification problem."],"metadata":{"id":"ZsER9ec_b8eX","executionInfo":{"status":"ok","timestamp":1647030840935,"user_tz":-345,"elapsed":14,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["###**Compile Keras Model**\n","- When we compile the Keras model, it uses the backend numerical libraries such as TensorFlow.\n","- When we are compiling the model we must specify some additional parameters to better evaluate the model and to find the best set of weights to map inputs to outputs.\n","\n","- **Loss Function** – one must specify the loss function to evaluate the set of weights on which model will be mapped. we will use cross-entropy as a loss function which is actually known as **binary cross-entropy** used for binary classification.\n","\n","- **Optimizer** – second is the optimizer to optimize the loss. we will use **adam** which is a popular version of gradient descent and gives the best result in most problems."],"metadata":{"id":"mcYekXg1eoS0"}},{"cell_type":"code","source":["model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"],"metadata":{"id":"ZE-P-0TpfNFr","executionInfo":{"status":"ok","timestamp":1647030845336,"user_tz":-345,"elapsed":630,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["###**Start Training (Fit the Model)**\n","\n","- After successful compilation of the model, we are ready to fit data to the model and start training the neural network.\n","- **Epoch**– How many time weight need to update by backpropagation process.\n","- **Batch size** – How many data  samples pass to the model before updating the weights in each time."],"metadata":{"id":"DG2QgSeEffwP"}},{"cell_type":"code","source":["model.fit(X_train,y_train ,epochs=150, batch_size=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SwIMpJQ0gusW","executionInfo":{"status":"ok","timestamp":1647030870503,"user_tz":-345,"elapsed":21549,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"13d659af-a95a-4172-8132-8a5a5bf8ff62"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","50/50 [==============================] - 1s 2ms/step - loss: 0.8274 - accuracy: 0.3647\n","Epoch 2/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.6875 - accuracy: 0.5671\n","Epoch 3/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.6477 - accuracy: 0.7134\n","Epoch 4/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.6244 - accuracy: 0.7435\n","Epoch 5/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.6043 - accuracy: 0.7735\n","Epoch 6/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.5849 - accuracy: 0.7796\n","Epoch 7/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.5649 - accuracy: 0.7796\n","Epoch 8/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.5460 - accuracy: 0.7836\n","Epoch 9/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.5271 - accuracy: 0.7856\n","Epoch 10/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.5059 - accuracy: 0.7816\n","Epoch 11/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.7836\n","Epoch 12/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4791 - accuracy: 0.7756\n","Epoch 13/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.7816\n","Epoch 14/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4612 - accuracy: 0.7836\n","Epoch 15/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4567 - accuracy: 0.7896\n","Epoch 16/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.7776\n","Epoch 17/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4488 - accuracy: 0.7836\n","Epoch 18/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.7876\n","Epoch 19/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.7856\n","Epoch 20/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.7856\n","Epoch 21/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4384 - accuracy: 0.7936\n","Epoch 22/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4359 - accuracy: 0.7896\n","Epoch 23/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4356 - accuracy: 0.7956\n","Epoch 24/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.7956\n","Epoch 25/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4299 - accuracy: 0.7976\n","Epoch 26/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4289 - accuracy: 0.7996\n","Epoch 27/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4254 - accuracy: 0.7996\n","Epoch 28/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4245 - accuracy: 0.8116\n","Epoch 29/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4235 - accuracy: 0.7976\n","Epoch 30/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4204 - accuracy: 0.8076\n","Epoch 31/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4206 - accuracy: 0.7996\n","Epoch 32/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.7956\n","Epoch 33/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4188 - accuracy: 0.8016\n","Epoch 34/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8076\n","Epoch 35/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4164 - accuracy: 0.7956\n","Epoch 36/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8016\n","Epoch 37/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4138 - accuracy: 0.7956\n","Epoch 38/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4105 - accuracy: 0.8056\n","Epoch 39/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8116\n","Epoch 40/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4078 - accuracy: 0.7996\n","Epoch 41/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4061 - accuracy: 0.8176\n","Epoch 42/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.8156\n","Epoch 43/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4037 - accuracy: 0.8116\n","Epoch 44/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.7976\n","Epoch 45/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4021 - accuracy: 0.8096\n","Epoch 46/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8116\n","Epoch 47/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3983 - accuracy: 0.8136\n","Epoch 48/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3979 - accuracy: 0.8076\n","Epoch 49/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3978 - accuracy: 0.8056\n","Epoch 50/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8076\n","Epoch 51/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8036\n","Epoch 52/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8076\n","Epoch 53/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3928 - accuracy: 0.8136\n","Epoch 54/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8036\n","Epoch 55/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8036\n","Epoch 56/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8156\n","Epoch 57/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8096\n","Epoch 58/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8056\n","Epoch 59/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3886 - accuracy: 0.8096\n","Epoch 60/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3887 - accuracy: 0.8116\n","Epoch 61/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8116\n","Epoch 62/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3869 - accuracy: 0.8096\n","Epoch 63/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8096\n","Epoch 64/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8116\n","Epoch 65/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.8036\n","Epoch 66/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3840 - accuracy: 0.8076\n","Epoch 67/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3837 - accuracy: 0.8156\n","Epoch 68/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8096\n","Epoch 69/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8056\n","Epoch 70/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3821 - accuracy: 0.8136\n","Epoch 71/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.8096\n","Epoch 72/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8156\n","Epoch 73/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3792 - accuracy: 0.8257\n","Epoch 74/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3771 - accuracy: 0.8176\n","Epoch 75/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3754 - accuracy: 0.8156\n","Epoch 76/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3764 - accuracy: 0.8216\n","Epoch 77/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3751 - accuracy: 0.8136\n","Epoch 78/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3731 - accuracy: 0.8156\n","Epoch 79/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3741 - accuracy: 0.8156\n","Epoch 80/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3731 - accuracy: 0.8196\n","Epoch 81/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8116\n","Epoch 82/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3702 - accuracy: 0.8216\n","Epoch 83/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8236\n","Epoch 84/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8236\n","Epoch 85/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3678 - accuracy: 0.8297\n","Epoch 86/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3669 - accuracy: 0.8297\n","Epoch 87/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8317\n","Epoch 88/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3667 - accuracy: 0.8236\n","Epoch 89/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3662 - accuracy: 0.8337\n","Epoch 90/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3671 - accuracy: 0.8176\n","Epoch 91/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8216\n","Epoch 92/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3631 - accuracy: 0.8236\n","Epoch 93/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3612 - accuracy: 0.8297\n","Epoch 94/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3612 - accuracy: 0.8337\n","Epoch 95/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3606 - accuracy: 0.8357\n","Epoch 96/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3618 - accuracy: 0.8277\n","Epoch 97/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.8297\n","Epoch 98/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3566 - accuracy: 0.8377\n","Epoch 99/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3561 - accuracy: 0.8377\n","Epoch 100/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8337\n","Epoch 101/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.8357\n","Epoch 102/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3554 - accuracy: 0.8277\n","Epoch 103/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8357\n","Epoch 104/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3540 - accuracy: 0.8317\n","Epoch 105/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.8417\n","Epoch 106/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8337\n","Epoch 107/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3556 - accuracy: 0.8317\n","Epoch 108/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.8317\n","Epoch 109/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.8317\n","Epoch 110/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3495 - accuracy: 0.8397\n","Epoch 111/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3498 - accuracy: 0.8337\n","Epoch 112/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8377\n","Epoch 113/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3464 - accuracy: 0.8397\n","Epoch 114/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3480 - accuracy: 0.8377\n","Epoch 115/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.8357\n","Epoch 116/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8417\n","Epoch 117/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3436 - accuracy: 0.8417\n","Epoch 118/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8417\n","Epoch 119/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3459 - accuracy: 0.8397\n","Epoch 120/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3424 - accuracy: 0.8397\n","Epoch 121/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3416 - accuracy: 0.8477\n","Epoch 122/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8417\n","Epoch 123/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3391 - accuracy: 0.8417\n","Epoch 124/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8497\n","Epoch 125/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8497\n","Epoch 126/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3407 - accuracy: 0.8417\n","Epoch 127/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8457\n","Epoch 128/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.8437\n","Epoch 129/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3365 - accuracy: 0.8477\n","Epoch 130/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8497\n","Epoch 131/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3355 - accuracy: 0.8557\n","Epoch 132/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3344 - accuracy: 0.8477\n","Epoch 133/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8497\n","Epoch 134/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3332 - accuracy: 0.8517\n","Epoch 135/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.8517\n","Epoch 136/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.8477\n","Epoch 137/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3339 - accuracy: 0.8457\n","Epoch 138/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8477\n","Epoch 139/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3327 - accuracy: 0.8557\n","Epoch 140/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3289 - accuracy: 0.8537\n","Epoch 141/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3300 - accuracy: 0.8537\n","Epoch 142/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3313 - accuracy: 0.8577\n","Epoch 143/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3231 - accuracy: 0.8637\n","Epoch 144/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3249 - accuracy: 0.8637\n","Epoch 145/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3258 - accuracy: 0.8537\n","Epoch 146/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3267 - accuracy: 0.8517\n","Epoch 147/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8437\n","Epoch 148/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.8597\n","Epoch 149/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.8477\n","Epoch 150/150\n","50/50 [==============================] - 0s 2ms/step - loss: 0.3242 - accuracy: 0.8597\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f730a548390>"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["###**Evaluate the Model**"],"metadata":{"id":"rMWbJCpahNZg"}},{"cell_type":"code","source":["accuracy = model.evaluate(X_test,y_test)\n","print(accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNghv381h6Bl","executionInfo":{"status":"ok","timestamp":1647030883990,"user_tz":-345,"elapsed":509,"user":{"displayName":"Tapendra baduwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14382986397241314178"}},"outputId":"bf21a3f4-cffd-4a4c-c04f-1d39f4d8345c"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["9/9 [==============================] - 0s 2ms/step - loss: 0.6342 - accuracy: 0.7323\n","[0.634177029132843, 0.732342004776001]\n"]}]}]}