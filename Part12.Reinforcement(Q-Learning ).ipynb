{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part12.Reinforcement(Q-Learning ).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOyc4QqB/36QuUfC63TXe71"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Reinforcement Learning(Q-Learning Algorithm)**\n","\n","-  In reinforcement Learning the **intelligent agent (computer program) learns automatically from an environment using feedbacks**  by performing the actions and seeing the results of actions. For each good action, the agent gets positive feedback, and for each bad action, the agent gets negative feedback or penalty.\n","\n","- The primary goal of an agent in reinforcement learning is to improve the performance by getting the maximum positive rewards.\n","\n","- Since there is no labeled data, so the agent is bound to learn by its experience only with the process of **hit and trial methods.**\n","\n","- Reinforcement Learning is now a big help in **recommendation systems like news, music apps, and web-series apps like Netflix, etc.** These apps work as per customer preferences.\n","\n","- Algorithms of Reinforcement Learning are:**Q-learning,SARSA,Deep Q-network etc.**"],"metadata":{"id":"qbZYYqToYwea"}},{"cell_type":"markdown","source":["##**Terms used in Reinforcement Learning**\n","- **Agent:** An entity that can perceive/explore the environment and act upon it.\n","- **Environment:** A situation in which an agent is present or surrounded.It can be anything such as a room, maze, football ground, etc\n","- **Action(a):** Actions are the moves taken by an agent within the environment.\n","- **State(s):** State is a situation returned by the environment after each action taken by the agent.\n","- **Reward(R):** A feedback returned to the agent from the environment to evaluate the action of the agent.\n","- **Policy:** Policy is a strategy applied by the agent for the next action based on the current state.\n","- **Value:** It is expected long-term retuned with the discount factor and opposite to the short-term reward.\n","- **Q-value:** It is mostly similar to the value, but it takes one additional parameter as a current action.\n","- **Value Function:** The value function gives information about how good the situation and action are and how much reward an agent can expect."],"metadata":{"id":"MS-UIiT9x4xs"}},{"cell_type":"markdown","source":["##**Q-Learning**\n","- Q-Learning is a Reinforcement learning policy that will find the next best action, given a current state. It chooses this action at random and aims to maximize the reward.\n","\n","- **Step 1:** Create an initial **Q-Table with all values initialized to 0.**\n","- **Step 2:** Choose an action and perform it. Update values in the table.\n","- **Step 3:** Get the value of the reward and calculate the value Q-Value using **Bellman Equation.**\n","- **Step 4:** Continue the same until the table is filled .\n","\n","\n","- Q-Learning is based on  **Bellman Equation** ,So we have to learn  **Bellman Equation** clearly at first."],"metadata":{"id":"4Ar56hg8UylK"}},{"cell_type":"markdown","source":["##**Bellman Equation**\n","\n","- It is associated with dynamic programming and used to calculate the values of a decision problem at a certain point by including the values of previous states.\n","- It is a way of calculating the value functions in dynamic programming or environment that leads to modern reinforcement learning.\n","\n","- **The key-elements used in Bellman equations are:**\n","\n","- Action performed by the agent is referred to as \"a\"\n","- State occurred by performing the action is \"s.\"\n","- The reward/feedback obtained for each good and bad action is \"R.\"\n","- A discount factor is Gamma \"γ.\"\n","\n","- **The Bellman equation can be written as:**\n","\n","$$V(s) = max [R(s,a) + γV(s`)]$$  \n","\n","- **Where,**\n","\n","- V(s)= value calculated at a particular point.\n","\n","- R(s,a) = Reward at a particular state s by performing an action.\n","\n","- γ = Discount factor\n","\n","- V(s`) = The value at the previous state.\n","\n","\n","- In the above equation, we are taking the max of the complete values because the agent tries to find the optimal solution always.\n","\n","- So now, using the Bellman equation, we will find value at each state of the given environment. We will start from the block, which is next to the target block.\n","\n","\n","\n","\n"],"metadata":{"id":"hrTNFuPLzLQU"}},{"cell_type":"markdown","source":["- From the figure we can say that ,the agent cannot cross the S6 block, as it is a solid wall. If the agent reaches the S4 block, then get the +1 reward, if it reaches the fire block, then gets -1 reward point. It can take four actions: move up, move down, move left, and move right.\n","\n","\n","<figure align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1eIfAycA-2-83vRAzgh8Mo4Pj8Etrlcbt\" height=\"300px\", width=\"400px\"> \n","</figure>\n","\n","- Lets our agent is at box number $S7$ Now, the agent has four options to move; if he moves to the blue box, then he will feel a bump if he moves to the fire box, then he will get the -1 reward ,other option are move up and move down. Now agent take positive reward and move up having higher  values of V(s).\n","\n","- **Calculation process of V(s):**\n","- V(s3) = max [R(s,a) + γV(s`)] ,  here V(s')= 0 because this  is first stat of move.\n","\n","- V(s3)= max[R(s,a)]=> V(s3)= max[1]=> V(s3)= 1.\n","\n","\n","- V(s2) = max [R(s,a) + γV(s`)], here γ= 0.9(lets), V(s')= 1(previous state value), and R(s, a)= 0, because there is no reward at this state.\n","\n","- V(s2)= max[0.9(1)]=> V(s)= max[0.9]=> V(s2) =0.9\n","- **Similarly update the value of V(s) in all box and take action.**\n","\n"],"metadata":{"id":"UX2-PCyt8G-d"}}]}